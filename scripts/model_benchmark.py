#!/usr/bin/env python3
"""
æ¨¡å‹å¯¹æ¯”æµ‹è¯•å·¥å…· - ç”¨äºå¯¹æ¯”ä¸åŒ LLM çš„ç¿»è¯‘å’Œæ‘˜è¦æ•ˆæœ

ä½¿ç”¨æ–¹æ³•:
    # ä½¿ç”¨çœŸå®é‡‡é›†æ•°æ®å¯¹æ¯”æ‰€æœ‰æ¨¡å‹
    python scripts/model_benchmark.py

    # åªæµ‹è¯•ç‰¹å®šæ¨¡å‹
    python scripts/model_benchmark.py --models qwen glm

    # é™åˆ¶æµ‹è¯•æ•°æ®æ¡æ•°
    python scripts/model_benchmark.py --limit 10

    # åªä½¿ç”¨ RSS æ•°æ®æº
    python scripts/model_benchmark.py --source rss
"""
import os
import sys
import json
import time
import argparse
from datetime import datetime
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, asdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

import litellm
litellm.suppress_debug_info = True

from src.config import Config
from src.collectors.rss import RSSCollector
from src.collectors.twitter import TwitterCollector
from src.collectors.youtube import YouTubeCollector
from src.prompts import PromptManager


# ============ æ¨¡å‹é…ç½® ============
MODELS = {
    "qwen": {
        "name": "Qwen3-8B (ç¡…åŸºæµåŠ¨)",
        "model": "openai/Qwen/Qwen3-8B",
        "api_base": "https://api.siliconflow.cn/v1",
        "api_key_env": "AI_API_KEY",
    },
    "glm": {
        "name": "GLM-4-Flash (æ™ºè°±)",
        "model": "openai/glm-4-flash",
        "api_base": "https://open.bigmodel.cn/api/paas/v4",
        "api_key_env": "ZAI_API_KEY",
    },
    # "deepseek": {
    #     "name": "DeepSeek-V3",
    #     "model": "deepseek/deepseek-chat",
    #     "api_base": "https://api.deepseek.com",
    #     "api_key_env": "DEEPSEEK_API_KEY",
    # },
}


def collect_real_data(limit_per_source: int = 5) -> Dict[str, List[str]]:
    """ä»é¡¹ç›®æ‰€æœ‰å¯ç”¨çš„é‡‡é›†å™¨è·å–çœŸå®æ•°æ®ï¼ŒæŒ‰æ¥æºåˆ†ç»„"""
    config_path = os.path.join(PROJECT_ROOT, "config", "config.yaml")
    config = Config(config_path)

    data_by_source = {}

    # RSS é‡‡é›†
    rss_config = config.get_source_config("rss")
    if rss_config.get("enabled"):
        print("ğŸ“¥ é‡‡é›† RSS æ•°æ®...")
        collector = RSSCollector(rss_config)
        items = collector.collect()
        titles = list(dict.fromkeys([item.title for item in items]))[:limit_per_source]
        if titles:
            data_by_source["RSS"] = titles
            print(f"   RSS: {len(titles)} æ¡")

    # Twitter é‡‡é›†
    twitter_config = config.get_source_config("twitter")
    if twitter_config.get("enabled"):
        print("ğŸ“¥ é‡‡é›† Twitter æ•°æ®...")
        collector = TwitterCollector(twitter_config)
        items = collector.collect()
        titles = list(dict.fromkeys([item.title for item in items]))[:limit_per_source]
        if titles:
            data_by_source["Twitter"] = titles
            print(f"   Twitter: {len(titles)} æ¡")

    # YouTube é‡‡é›†
    youtube_config = config.get_source_config("youtube")
    if youtube_config.get("enabled"):
        print("ğŸ“¥ é‡‡é›† YouTube æ•°æ®...")
        collector = YouTubeCollector(youtube_config)
        items = collector.collect()
        titles = list(dict.fromkeys([item.title for item in items]))[:limit_per_source]
        if titles:
            data_by_source["YouTube"] = titles
            print(f"   YouTube: {len(titles)} æ¡")

    return data_by_source


@dataclass
class TestResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    model_key: str
    model_name: str
    source: str  # æ•°æ®æ¥æº: RSS, Twitter, YouTube
    title: str
    translated: str
    summary: str
    latency_ms: float
    success: bool
    error: Optional[str] = None


@dataclass
class ModelStats:
    """æ¨¡å‹ç»Ÿè®¡æ•°æ®"""
    model_key: str
    model_name: str
    total_tests: int
    success_count: int
    fail_count: int
    avg_latency_ms: float
    min_latency_ms: float
    max_latency_ms: float


# åˆå§‹åŒ– PromptManager
prompt_manager = PromptManager()


def build_prompt(titles: List[str], model: str = None) -> Dict[str, str]:
    """æ„å»ºæµ‹è¯•æç¤ºè¯ï¼Œè¿”å› system å’Œ user prompt"""
    content = prompt_manager.format_content_list(titles)
    return prompt_manager.get_prompt(
        task_name='translate_summarize',
        model=model,
        variables={'content': content}
    )


def parse_response(text: str, count: int) -> List[Dict]:
    """è§£ææ¨¡å‹å“åº”"""
    try:
        start = text.find('[')
        end = text.rfind(']') + 1
        if start >= 0 and end > start:
            return json.loads(text[start:end])
    except:
        pass
    return [{"index": i+1, "translated": "", "summary": ""} for i in range(count)]


def test_model(model_key: str, config: Dict, titles: List[str], source: str) -> List[TestResult]:
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    results = []
    api_key = os.environ.get(config["api_key_env"], "")

    if not api_key:
        print(f"  [è·³è¿‡] æœªè®¾ç½®ç¯å¢ƒå˜é‡ {config['api_key_env']}")
        for title in titles:
            results.append(TestResult(
                model_key=model_key,
                model_name=config["name"],
                source=source,
                title=title,
                translated="",
                summary="",
                latency_ms=0,
                success=False,
                error=f"Missing {config['api_key_env']}"
            ))
        return results

    prompts = build_prompt(titles, config["model"])

    start_time = time.time()
    try:
        messages = []
        if prompts['system']:
            messages.append({"role": "system", "content": prompts['system']})
        messages.append({"role": "user", "content": prompts['user']})

        kwargs = {
            "model": config["model"],
            "messages": messages,
            "api_key": api_key,
            "max_tokens": 2000,
        }
        if config.get("api_base"):
            kwargs["api_base"] = config["api_base"]

        response = litellm.completion(**kwargs)
        latency_ms = (time.time() - start_time) * 1000

        result_text = response.choices[0].message.content
        parsed = parse_response(result_text, len(titles))

        for i, title in enumerate(titles):
            item = next((p for p in parsed if p.get("index") == i+1), {})
            results.append(TestResult(
                model_key=model_key,
                model_name=config["name"],
                source=source,
                title=title,
                translated=item.get("translated", ""),
                summary=item.get("summary", ""),
                latency_ms=latency_ms / len(titles),
                success=True,
            ))

    except Exception as e:
        latency_ms = (time.time() - start_time) * 1000
        for title in titles:
            results.append(TestResult(
                model_key=model_key,
                model_name=config["name"],
                source=source,
                title=title,
                translated="",
                summary="",
                latency_ms=latency_ms / len(titles),
                success=False,
                error=str(e)
            ))

    return results


def calculate_stats(results: List[TestResult]) -> ModelStats:
    """è®¡ç®—æ¨¡å‹ç»Ÿè®¡æ•°æ®"""
    success_results = [r for r in results if r.success]
    latencies = [r.latency_ms for r in success_results] if success_results else [0]

    return ModelStats(
        model_key=results[0].model_key,
        model_name=results[0].model_name,
        total_tests=len(results),
        success_count=len(success_results),
        fail_count=len(results) - len(success_results),
        avg_latency_ms=sum(latencies) / len(latencies),
        min_latency_ms=min(latencies),
        max_latency_ms=max(latencies),
    )


def print_comparison(all_results: Dict[str, List[TestResult]]):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    print("\n" + "=" * 80)
    print("æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("=" * 80)

    # ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<25} {'æˆåŠŸç‡':<12} {'å¹³å‡å»¶è¿Ÿ':<15} {'çŠ¶æ€'}")
    print("-" * 60)

    for model_key, results in all_results.items():
        stats = calculate_stats(results)
        success_rate = f"{stats.success_count}/{stats.total_tests}"
        latency = f"{stats.avg_latency_ms:.0f}ms" if stats.success_count > 0 else "N/A"
        status = "âœ…" if stats.success_count == stats.total_tests else "âš ï¸" if stats.success_count > 0 else "âŒ"
        print(f"{stats.model_name:<25} {success_rate:<12} {latency:<15} {status}")

    # æŒ‰æ¥æºåˆ†ç»„æ˜¾ç¤ºè¯¦ç»†å¯¹æ¯”
    sources = list(set(r.source for results in all_results.values() for r in results))

    for source in sources:
        print(f"\nğŸ“ [{source}] ç¿»è¯‘å¯¹æ¯”:")
        print("-" * 80)

        # è·å–è¯¥æ¥æºçš„æ ‡é¢˜ï¼ˆå–å‰2æ¡ï¼‰
        titles = []
        for results in all_results.values():
            for r in results:
                if r.source == source and r.title not in titles:
                    titles.append(r.title)
                    if len(titles) >= 2:
                        break
            if len(titles) >= 2:
                break

        for title in titles:
            # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
            display_title = title[:60] + "..." if len(title) > 60 else title
            print(f"\nåŸæ–‡: {display_title}")
            for model_key, results in all_results.items():
                result = next((r for r in results if r.title == title), None)
                if result and result.success:
                    print(f"  [{MODELS[model_key]['name'][:12]}]")
                    print(f"    ç¿»è¯‘: {result.translated}")
                    print(f"    æ‘˜è¦: {result.summary}")
                else:
                    error = result.error if result else "æœªæµ‹è¯•"
                    print(f"  [{MODELS[model_key]['name'][:12]}] âŒ {error}")


def save_results(all_results: Dict[str, List[TestResult]], output_path: str):
    """ä¿å­˜ç»“æœåˆ° JSON"""
    data = {
        "timestamp": datetime.now().isoformat(),
        "results": {k: [asdict(r) for r in v] for k, v in all_results.items()},
        "stats": {k: asdict(calculate_stats(v)) for k, v in all_results.items()},
    }
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹å¯¹æ¯”æµ‹è¯•å·¥å…·")
    parser.add_argument("--models", nargs="+", help="è¦æµ‹è¯•çš„æ¨¡å‹ (é»˜è®¤å…¨éƒ¨)")
    parser.add_argument("--limit", type=int, default=5, help="æ¯ä¸ªæ•°æ®æºçš„æµ‹è¯•æ¡æ•° (é»˜è®¤5)")
    parser.add_argument("--output", default="tests/results/benchmark_results.json", help="è¾“å‡ºæ–‡ä»¶")
    args = parser.parse_args()

    # ç¡®å®šè¦æµ‹è¯•çš„æ¨¡å‹
    model_keys = args.models if args.models else list(MODELS.keys())
    model_keys = [k for k in model_keys if k in MODELS]

    if not model_keys:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹é…ç½®")
        return

    # é‡‡é›†çœŸå®æ•°æ®ï¼ˆæŒ‰æ¥æºåˆ†ç»„ï¼‰
    print("ğŸš€ æ¨¡å‹å¯¹æ¯”æµ‹è¯•\n")
    data_by_source = collect_real_data(limit_per_source=args.limit)

    if not data_by_source:
        print("âŒ æœªèƒ½é‡‡é›†åˆ°æµ‹è¯•æ•°æ®")
        return

    total_count = sum(len(titles) for titles in data_by_source.values())
    print(f"\nğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   æ¨¡å‹: {', '.join(model_keys)}")
    print(f"   æ•°æ®æº: {', '.join(data_by_source.keys())}")
    print(f"   æ€»æ•°æ®: {total_count} æ¡")
    print()

    # è¿è¡Œæµ‹è¯•ï¼ˆæŒ‰æ¥æºåˆ†æ‰¹æµ‹è¯•ï¼‰
    all_results = {k: [] for k in model_keys}

    for source, titles in data_by_source.items():
        print(f"\nğŸ”„ æµ‹è¯• [{source}] æ•°æ® ({len(titles)} æ¡)...")
        for model_key in model_keys:
            config = MODELS[model_key]
            print(f"   ğŸ“¡ {config['name']}...")
            results = test_model(model_key, config, titles, source)
            all_results[model_key].extend(results)

            success = sum(1 for r in results if r.success)
            print(f"      å®Œæˆ: {success}/{len(results)} æˆåŠŸ")

    # è¾“å‡ºç»“æœ
    print_comparison(all_results)
    save_results(all_results, args.output)


if __name__ == "__main__":
    main()
